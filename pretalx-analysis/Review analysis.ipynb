{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713915ce-78e1-4d1d-bc4c-5c3465d6623d",
   "metadata": {},
   "source": [
    "# Analyzing and ordering reviews for proposal selection\n",
    "\n",
    "Every year, SciPy gets $N$ proposals, yet must restrict its selection to $M << N$ for each of its track.\n",
    "Selecting these is a tricky exercise, especially when trying to avoid one's own biases.\n",
    "One useful way of doing so is to rely on review scores provided by reviewers to list proposals by decreasing order of presumed importance.\n",
    "However, despite the guidelines on scoring reviews, reviewers don't all score the same way.\n",
    "Some score high on average, making their top scores difficult to leverage as strong statements of quality regarding a paper.\n",
    "Others score low on average, with the similar but opposite results.\n",
    "Yet others make a deliberate effort to spread their scores across the whole $[-10, 10]$ scale, trying to align some notion of score centrality to 0.\n",
    "\n",
    "In this notebook, we endeavor to emulate the latter scorers,\n",
    "providing a projection of every review scores on a zero-centered scale with respect to the set of scores specific to each reviewer.\n",
    "Aligning the median score of every reviewer to zero, we compute a linear interpolation of each of their scores so that 1.0 and -1.0 match either their $0.75$ or $0.25$ quantile respectively.\n",
    "The result is a measurement of how amazed or disappointed a reviewer is with respect to their median appreciation of a proposal.\n",
    "\n",
    "Remark that such score projections are not an authoritative recasting of reviewer scores.\n",
    "We should heed reviewer comments, and still look at their nominal scores.\n",
    "However, the mean score projection for each review turns out to be useful in a pivot table analysis,\n",
    "enabling a conference chair to list proposals by decreasing order of how _wowed_ reviewers were by papers on average,\n",
    "and this can make the selection process a little easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913ab7e8-c133-4c96-82b8-5b6dcd1980f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv pandas pyarrow requests tqdm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e6e33-97ba-4db7-8c75-c4ca914d2a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import closing\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests as rq\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f14102-c83d-44e2-8252-26163d572f99",
   "metadata": {},
   "source": [
    "We get the data necessary for this work out of Pretalx, to which we authenticate using our API token.\n",
    "In order to avoid writing up tokens in notebook code\n",
    "(which I keep forgetting to take out before committing),\n",
    "let's write it up in a file named `.env` in the current directory.\n",
    "All this file needs to contain is the following:\n",
    "\n",
    "```\n",
    "TOKEN = \"API token copy-pasted out of one's Pretalx profile\"\n",
    "```\n",
    "\n",
    "Fetch the token from [here](https://cfp.scipy.org/orga/me),\n",
    "then use the following `%%writefile` cell magic to set it up once\n",
    "(by turning it to a code cell).\n",
    "Once done, make it a raw cell again and carry on."
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc94c6b2-f7cb-4d97-bb18-bdb8d0f0550f",
   "metadata": {},
   "source": [
    "%%writefile .env\n",
    "TOKEN = \"Insert token here!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c7273c-3a05-4f42-9f76-2ae7e6c31363",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be0b923-b148-42e1-ae3d-a7d763cd6c30",
   "metadata": {},
   "source": [
    "Check here that the token was properly loaded.\n",
    "On error, check that you effectively wrote up a file named `.env` in the current directory,\n",
    "and that this file contains the definition of a `TOKEN` as described above.\n",
    "If everything looks fine, run the previous cell again to retry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aada0bd-17b9-488f-9ffe-ea87aaeb775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = os.environ[\"TOKEN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1242ab-a638-49b8-928f-8db4fe3c1854",
   "metadata": {},
   "source": [
    "## Fetch proposals and reviews\n",
    "\n",
    "Both proposals (_submissions_ in Pretalx talx) and reviews are accessible through HTTP request/response streams through the Pretalx API.\n",
    "Let's fetch all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95562dd3-2244-4d6f-9006-d095fbe43cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sequence_cfp_scipy(url1, max_queries=50):\n",
    "    sequence = []\n",
    "    url = url1\n",
    "    max_queries = 50\n",
    "    num_queries = 0\n",
    "    num_results_expected = None\n",
    "\n",
    "    with closing(tqdm(total=max_queries)) as progress:\n",
    "        while True:\n",
    "            response = rq.get(url, headers={\"Authorization\": f\"Token {TOKEN}\"})\n",
    "            assert response.ok\n",
    "            data = response.json()\n",
    "            progress.update()\n",
    "            num_queries += 1\n",
    "\n",
    "            assert \"results\" in data\n",
    "            assert \"next\" in data\n",
    "\n",
    "            if num_results_expected is None and \"count\" in data:\n",
    "                num_results_expected = data[\"count\"]\n",
    "                max_queries = int(np.ceil(num_results_expected / len(data[\"results\"])))\n",
    "                progress.reset(max_queries)\n",
    "                progress.update(num_queries)\n",
    "            else:\n",
    "                assert num_results_expected == data[\"count\"]\n",
    "\n",
    "            sequence += data[\"results\"]\n",
    "            url = data[\"next\"]\n",
    "            if not url:\n",
    "                break\n",
    "\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65feadf-ec6b-44d0-b827-a9d9ab671719",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_ = fetch_sequence_cfp_scipy(\n",
    "    \"https://cfp.scipy.org/api/events/2024/submissions/\"\n",
    ")\n",
    "len(submissions_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd3a5ad-3d47-418b-a664-3dca92ce8399",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_ = fetch_sequence_cfp_scipy(\"https://cfp.scipy.org/api/events/2024/reviews/\")\n",
    "len(reviews_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac7cf5-39ee-4ea1-a7a9-f843eeec456e",
   "metadata": {},
   "source": [
    "Submissions are delivered in JSON form, let's make them into a Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb325b-2c23-4f5a-a027-6410c24e04c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = pd.DataFrame.from_records(submissions_)\n",
    "submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef05fe5-bb75-4e28-806c-73c4e932a25b",
   "metadata": {},
   "source": [
    "Bit of data clean-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d76651e-ad47-44d6-9140-02f173287789",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions[\"submission_type\"] = submissions[\"submission_type\"].apply(\n",
    "    lambda x: x[\"en\"] if isinstance(x, dict) else x\n",
    ")\n",
    "submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc5b540-bfed-427f-9659-01def9919639",
   "metadata": {},
   "source": [
    "Now, reviews, along with its own data clean-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be83ee-1543-4fc5-95b7-94ea03771e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.DataFrame.from_records(reviews_)\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d44c67-525e-4612-9e93-404e337e5a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"score\"] = reviews[\"score\"].map(float)\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d871208-3f4a-4531-91f9-462ccfc8b95c",
   "metadata": {},
   "source": [
    "## Compute score projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db17bc1-9c5e-41c6-90a2-9de1e742218c",
   "metadata": {},
   "source": [
    "The score projection scheme described [above](#score-projection) involves computing the $(0.25, 0.5, 0.75)$ quantiles of review scores, grouped by reviewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb74ad-9b36-4b7e-ba40-0046da7dd18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_quantiles = (\n",
    "    reviews.groupby(\"user\", as_index=False)\n",
    "    .agg({\"score\": lambda g: list(g.quantile(q=[0.25, 0.5, 0.75]))})\n",
    "    .rename(columns={\"score\": \"quantiles\"})\n",
    ")\n",
    "score_quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6efe8e-a93b-4d2f-b083-2a17cd1383f9",
   "metadata": {},
   "source": [
    "Compute score projections and append them to other relevant review parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85baf3be-097b-46cf-b848-42af59f064ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_by_quantiles(score, q_low, med, q_up):\n",
    "    if score <= med:\n",
    "        if med == q_low:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return (score - med) / (med - q_low)\n",
    "    else:\n",
    "        if med == q_up:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return (score - med) / (q_up - med)\n",
    "\n",
    "\n",
    "reviews_proj = reviews[[\"submission\", \"text\", \"user\", \"score\"]].merge(\n",
    "    score_quantiles, on=\"user\"\n",
    ")\n",
    "reviews_proj[\"projection\"] = [\n",
    "    normalize_by_quantiles(score, q_low, med, q_up)\n",
    "    for score, (q_low, med, q_up) in reviews_norm[[\"score\", \"quantiles\"]].itertuples(\n",
    "        index=False\n",
    "    )\n",
    "]\n",
    "reviews_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae18672b-c15a-4938-b82b-a63944cd2022",
   "metadata": {},
   "source": [
    "And now join submissions to reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa892f39-838f-4209-803f-45f44a1ef24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_reviewed = (\n",
    "    submissions.assign(\n",
    "        authors=submissions[\"speakers\"].map(lambda x: \", \".join(a[\"name\"] for a in x))\n",
    "    )[[\"code\", \"authors\", \"title\"]]\n",
    "    .merge(\n",
    "        reviews_proj[[\"submission\", \"user\", \"text\", \"score\", \"projection\"]],\n",
    "        how=\"left\",\n",
    "        left_on=\"code\",\n",
    "        right_on=\"submission\",\n",
    "    )\n",
    "    .drop(columns=[\"submission\"])\n",
    ")\n",
    "submissions_reviewed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a90d45d-e402-418b-9a3a-0be3cef79b71",
   "metadata": {},
   "source": [
    "## Eyeball it all 👀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda66465-726b-4a8c-8f44-53f2f085e1c6",
   "metadata": {},
   "source": [
    "To display submissions in some order of aggregate score or projection, compute these aggregates over `submissions_reviewed` and use `.sort_values()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c649c06-13fa-49d8-bb77-44df9dbc1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_reviewed_agg = (\n",
    "    submissions_reviewed.groupby([\"code\", \"authors\", \"title\"], as_index=False)\n",
    "    .agg({\"text\": \"count\", \"score\": \"median\", \"projection\": \"mean\"})\n",
    "    .rename(columns={\"text\": \"num_reviews\"})\n",
    ")\n",
    "submissions_reviewed_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e58a6-f3ae-473f-8bb1-21196d07de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_reviewed_agg.sort_values(\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ed113-ab0f-426e-8bd7-016bbb7bf6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_reviewed_agg.sort_values(\"projection\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfadc433-9a56-4f17-a5ab-877361da03a9",
   "metadata": {},
   "source": [
    "Do we have any submissions for which we don't have a review?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc77299c-b7cf-4cbc-a194-d11ae1774eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_reviewed_agg.loc[submissions_reviewed_agg[\"score\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e60ffc-c91c-4c52-b89a-235643c4fe3d",
   "metadata": {},
   "source": [
    "One may go and check in Pretalx whether these submissions got reviewed, and if they do have any, get on with debugging.\n",
    "One funny quirk is that if the user running the analysis has submitted a proposal,\n",
    "this user is barred from seeing the raw reviews their proposal received.\n",
    "Hence, their proposal will show up in the previous listing as if it went unreviewed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
